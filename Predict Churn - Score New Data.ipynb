{
    "nbformat": 4, 
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Customer Churn Model Scoring\n#### The objectives of this lab is:\n- score **new** customer data against a pre-built model\n- schedule the notebook to run via the Notebook scheduler\n\n### Step 1: Download new customer data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "-rw------- 1 s3b2-c7634938ff52ab-a5f39cf201a0 users 27597 Nov 13 09:17 new_customer_churn_data.csv\r\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "cell_type": "code", 
            "source": "import wget\nurl_customer='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/new_customer_churn_data.csv'\n\n#remove existing files before downloading\n!rm -f new_customer_churn_data.csv\n\ncustomerFilename=wget.download(url_customer)\n\n!ls -l new_customer_churn_data.csv"
        }, 
        {
            "source": "### Step 2: Read data into a Spark DataFrame\n**Note**: the new dataset does not contain the label column", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nnewData= sqlContext.read\\\n    .format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\")\\\n    .option(\"header\", \"true\")\\\n    .option(\"inferSchema\", \"true\")\\\n    .load(customerFilename)"
        }, 
        {
            "execution_count": 3, 
            "metadata": {}, 
            "outputs": [
                {
                    "data": {
                        "text/plain": "     ID Gender Status  Children  EstIncome CarOwner        Age  LongDistance  \\\n0  2048      F      S         1    13576.5        N  39.426667         14.83   \n1  2054      F      M         2    84166.1        N  54.013333          3.28   \n2  2075      F      S         0    68427.4        N  42.393333         23.76   \n3  2095      F      M         2    77551.1        Y  33.600000         20.53   \n4  2108      F      S         1    13109.1        N  62.606667         22.38   \n\n   International  Local  Dropped Paymethod LocalBilltype LongDistanceBilltype  \\\n0              0  25.66        0        CC        Budget             Standard   \n1              0  11.74        1        CC        Budget             Standard   \n2              0  50.05        0      Auto     FreeLocal             Standard   \n3              0  41.89        1        CC        Budget       Intnl_discount   \n4              0  40.48        0      Auto        Budget             Standard   \n\n   Usage  RatePlan  \n0  40.49         1  \n1  15.02         2  \n2  73.81         3  \n3  62.42         2  \n4  62.87         1  ", 
                        "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Gender</th>\n      <th>Status</th>\n      <th>Children</th>\n      <th>EstIncome</th>\n      <th>CarOwner</th>\n      <th>Age</th>\n      <th>LongDistance</th>\n      <th>International</th>\n      <th>Local</th>\n      <th>Dropped</th>\n      <th>Paymethod</th>\n      <th>LocalBilltype</th>\n      <th>LongDistanceBilltype</th>\n      <th>Usage</th>\n      <th>RatePlan</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2048</td>\n      <td>F</td>\n      <td>S</td>\n      <td>1</td>\n      <td>13576.5</td>\n      <td>N</td>\n      <td>39.426667</td>\n      <td>14.83</td>\n      <td>0</td>\n      <td>25.66</td>\n      <td>0</td>\n      <td>CC</td>\n      <td>Budget</td>\n      <td>Standard</td>\n      <td>40.49</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2054</td>\n      <td>F</td>\n      <td>M</td>\n      <td>2</td>\n      <td>84166.1</td>\n      <td>N</td>\n      <td>54.013333</td>\n      <td>3.28</td>\n      <td>0</td>\n      <td>11.74</td>\n      <td>1</td>\n      <td>CC</td>\n      <td>Budget</td>\n      <td>Standard</td>\n      <td>15.02</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2075</td>\n      <td>F</td>\n      <td>S</td>\n      <td>0</td>\n      <td>68427.4</td>\n      <td>N</td>\n      <td>42.393333</td>\n      <td>23.76</td>\n      <td>0</td>\n      <td>50.05</td>\n      <td>0</td>\n      <td>Auto</td>\n      <td>FreeLocal</td>\n      <td>Standard</td>\n      <td>73.81</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2095</td>\n      <td>F</td>\n      <td>M</td>\n      <td>2</td>\n      <td>77551.1</td>\n      <td>Y</td>\n      <td>33.600000</td>\n      <td>20.53</td>\n      <td>0</td>\n      <td>41.89</td>\n      <td>1</td>\n      <td>CC</td>\n      <td>Budget</td>\n      <td>Intnl_discount</td>\n      <td>62.42</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2108</td>\n      <td>F</td>\n      <td>S</td>\n      <td>1</td>\n      <td>13109.1</td>\n      <td>N</td>\n      <td>62.606667</td>\n      <td>22.38</td>\n      <td>0</td>\n      <td>40.48</td>\n      <td>0</td>\n      <td>Auto</td>\n      <td>Budget</td>\n      <td>Standard</td>\n      <td>62.87</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    }, 
                    "execution_count": 3, 
                    "output_type": "execute_result", 
                    "metadata": {}
                }
            ], 
            "cell_type": "code", 
            "source": "newData = newData.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\nnewData.toPandas().head()"
        }, 
        {
            "source": "### Step 3: Load Saved Model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "from pyspark.ml import PipelineModel\nmodel1_loaded = PipelineModel.load(\"PredictChurn.churnModel\")"
        }, 
        {
            "source": "### Step 4: Score the new data\nNote: The scored output contains the predicted values and confidence scores", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "source": "result = model1_loaded.transform(newData)"
        }, 
        {
            "source": "### Step 5: Export Score into a csv file", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "+----+--------------+----------+-----------------------------------------+\n|ID  |predictedLabel|prediction|probability                              |\n+----+--------------+----------+-----------------------------------------+\n|2048|T             |1.0       |[0.017391304347826087,0.9826086956521738]|\n|2054|T             |1.0       |[0.39325700280112047,0.6067429971988795] |\n|2075|F             |0.0       |[0.942270779314416,0.05772922068558384]  |\n|2095|F             |0.0       |[0.9763728885493592,0.023627111450640864]|\n|2108|T             |1.0       |[0.075,0.925]                            |\n+----+--------------+----------+-----------------------------------------+\nonly showing top 5 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "cell_type": "code", 
            "source": "#Select ID, prediction and probability fields from the result dataframe\n\nr1=result.select(result[\"ID\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\nr1.show(5,False)"
        }, 
        {
            "source": "#### Decompose the probability column\nThe probability column contains a vector for each record, and the elements must be extracted", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "metadata": {}, 
            "outputs": [
                {
                    "text": "+----+----------+--------------------+--------------------+\n|ID  |prediction|probability_0       |probability_1       |\n+----+----------+--------------------+--------------------+\n|2048|1.0       |0.017391304347826087|0.9826086956521738  |\n|2054|1.0       |0.39325700280112047 |0.6067429971988795  |\n|2075|0.0       |0.942270779314416   |0.05772922068558384 |\n|2095|0.0       |0.9763728885493592  |0.023627111450640864|\n|2108|1.0       |0.075               |0.925               |\n|2124|0.0       |0.9919057692664552  |0.00809423073354488 |\n|2154|1.0       |0.16981225296442687 |0.8301877470355731  |\n|2218|0.0       |0.9143637805237788  |0.08563621947622133 |\n|2267|0.0       |0.9805743411792578  |0.01942565882074223 |\n|2284|1.0       |0.08849206349206348 |0.9115079365079366  |\n+----+----------+--------------------+--------------------+\nonly showing top 10 rows\n\n", 
                    "name": "stdout", 
                    "output_type": "stream"
                }
            ], 
            "cell_type": "code", 
            "source": "from pyspark.sql import Row\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import udf\nfrom pyspark.ml.linalg import Vectors\n\nudf_0 = udf(lambda vector: float(vector[0]), DoubleType())\nudf_1 = udf(lambda vector: float(vector[1]), DoubleType())\n\nr2 = (r1.select(r1[\"ID\"], r1[\"prediction\"],r1[\"probability\"])\n    .withColumn('probability_0', udf_0(r1.probability))\n    .withColumn('probability_1', udf_1(r1.probability))\n    .drop(\"probability\"))\n\nr2.show(10, False)"
        }
    ], 
    "metadata": {
        "language_info": {
            "nbconvert_exporter": "python", 
            "file_extension": ".py", 
            "name": "python", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "mimetype": "text/x-python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }
    }
}